{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TF-IDF Analysis of Presidential Candidates\n",
        "\n",
        "This notebook performs a TF-IDF (Term Frequency-Inverse Document Frequency) analysis on speeches by Donald Trump, Kamala Harris, and Joe Biden. The goal is to identify distinctive keywords and themes for each candidate by treating all their speeches as a single document and comparing them against each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e7c92f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re\n",
        "\n",
        "# Set plot style\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (14, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebff548a",
      "metadata": {},
      "source": [
        "## 1. Setup and Data Loading\n",
        "\n",
        "We load the speech data for all three candidates. For Trump, we use the cleaned transcriptions available in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b80b80f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add src to path to import project modules\n",
        "# Assuming notebook is in notebooks/NLP/\n",
        "project_root = Path.cwd().parent.parent\n",
        "if str(project_root / \"src\") not in sys.path:\n",
        "    sys.path.append(str(project_root / \"src\"))\n",
        "\n",
        "from filtering_corpus.speech_corpus import SpeechCorpus\n",
        "from filtering_corpus.other_candidates import OtherCandidatesCorpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d1326a",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading Trump speeches...\")\n",
        "corpus = SpeechCorpus(data_dir=str(project_root / \"data\"), transcription_file=\"transcriptions.parquet\")\n",
        "# Filter recent speeches only? Or all? Let's take all processed ones.\n",
        "# We strictly need ones with 'clean-v1-with-stopwords' available, which should be all in the parquet if processed.\n",
        "# Using 'clean-v1-with-stopwords' as the text source\n",
        "df_trump = corpus.get_full_speeches(text_columns=['clean-v1-with-stopwords'])\n",
        "df_trump['candidate'] = 'Trump'\n",
        "# Rename to a standard 'text' column for analysis\n",
        "df_trump.rename(columns={'clean-v1-with-stopwords': 'text'}, inplace=True)\n",
        "\n",
        "print(f\"Loaded {len(df_trump)} Trump speeches.\")\n",
        "\n",
        "print(\"Loading Biden and Harris speeches...\")\n",
        "other_corpus = OtherCandidatesCorpus(data_dir=str(project_root / \"data\"))\n",
        "\n",
        "kamala_corpus = other_corpus.get_kamala()\n",
        "df_kamala = kamala_corpus.get_full_speeches(text_columns=['cleaned_transcription'])\n",
        "df_kamala['candidate'] = 'Harris'\n",
        "df_kamala.rename(columns={'cleaned_transcription': 'text'}, inplace=True)\n",
        "\n",
        "biden_corpus = other_corpus.get_biden()\n",
        "df_biden = biden_corpus.get_full_speeches(text_columns=['cleaned_transcription'])\n",
        "df_biden['candidate'] = 'Biden'\n",
        "df_biden.rename(columns={'cleaned_transcription': 'text'}, inplace=True)\n",
        "\n",
        "print(f\"Loaded {len(df_kamala)} Harris speeches.\")\n",
        "print(f\"Loaded {len(df_biden)} Biden speeches.\")\n",
        "\n",
        "# Combine all\n",
        "df_all = pd.concat([df_trump[['text', 'candidate']], \n",
        "                    df_kamala[['text', 'candidate']], \n",
        "                    df_biden[['text', 'candidate']]], ignore_index=True)\n",
        "\n",
        "# Drop rows with missing text\n",
        "df_all = df_all.dropna(subset=['text'])\n",
        "print(f\"Total speeches: {len(df_all)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77203aaa",
      "metadata": {},
      "source": [
        "## 2. TF-IDF Analysis\n",
        "\n",
        "To find the most distinctive terms for each candidate, we will:\n",
        "1. Concatenate all speeches for each candidate into one massive text blob.\n",
        "2. Treat these 3 blobs as our \"corpus\" of 3 documents.\n",
        "3. Apply TF-IDF. Terms that appear frequently in one candidate's speeches but rarely in the others' will have high scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12db90f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Group by candidate and join text\n",
        "candidate_docs = df_all.groupby('candidate')['text'].apply(lambda x: \" \".join(x)).reset_index()\n",
        "\n",
        "# Clean up specific artifacts requested by user\n",
        "# We remove transcription annotations like [APPLAUSE], [Cheers], etc. \n",
        "# and also handle some tokenization artifacts if they appear in the raw text.\n",
        "\n",
        "def additional_cleaning(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Remove bracketed content like [APPLAUSE] if present\n",
        "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
        "    # Remove explicit words that might have escaped brackets\n",
        "    text = re.sub(r'\\b(applause|cheers|audience|laughter)\\b', '', text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "candidate_docs['text'] = candidate_docs['text'].apply(additional_cleaning)\n",
        "\n",
        "candidates = candidate_docs['candidate'].tolist()\n",
        "docs = candidate_docs['text'].tolist()\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "# Custom stop words to handle tokenization artifacts ('gon', 'na', 'ca') and common contractions\n",
        "custom_stop_words = list(TfidfVectorizer(stop_words='english').get_stop_words()) + [\n",
        "    'gon', 'na', 'ca', 've', 'll', 're', 'don', 'won', 'didn', 'doesn'\n",
        "]\n",
        "\n",
        "# We can also limit max_features if we want, but let's keep it open for now.\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=custom_stop_words, ngram_range=(1, 2), max_df=0.7)\n",
        "\n",
        "# Fit and transform\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
        "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eed3aee",
      "metadata": {},
      "source": [
        "### Methodology: Why Concatenate Speeches?\n",
        "\n",
        "We concatenate all speeches for a single candidate into one large \"document\" for two main reasons:\n",
        "1.  **Candidate-Level Analysis**: We are interested in the vocabulary that characterizes *the candidate*, not specific speeches. By aggregating, we treat the candidate's entire corpus as their linguistic footprint.\n",
        "2.  **TF-IDF Context**: In this setup, \"Term Frequency\" (TF) becomes the frequency of a word in the candidate's total output. \"Inverse Document Frequency\" (IDF) penalizes words that appear in *all three candidates'* corpora. A high score therefore identifies words that are **frequent for this candidate but rare for the others**.\n",
        "\n",
        "### Impact of Data Imbalance\n",
        "\n",
        "We have a significant imbalance: **888 Trump speeches** vs. **40 Biden** and **18 Harris**. \n",
        "-   **Corpus Size**: Trump's concatenated document is vastly larger. \n",
        "-   **TF Normalization**: TF-IDF typically normalizes for document length (using L2 normalization by default in sklearn). This means the raw count doesn't matter as much as the *proportion* of the word in the document.\n",
        "-   **Vocabulary Richness**: However, a larger corpus is more likely to contain a wider variety of words. This might make Trump's top terms more stable, whereas Harris's might be driven by specific topics she repeated often in her few available speeches.\n",
        "-   **Noise**: Trump's larger corpus might include more noise or \"long tail\" words, but the top TF-IDF terms are usually robust to this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3989a593",
      "metadata": {},
      "source": [
        "### Interpreting TF-IDF Scores\n",
        "\n",
        "-   **High Score**: The term is highly characteristic of this candidate. It appears frequently in their speeches and rarely (or less frequently relative to overall usage) in the opponents'.\n",
        "-   **Stop Words Removal**: We effectively removed terms like \"gon\", \"na\", \"ca\" (artifacts of \"gonna\", \"wanna\", \"can't\") to focus on meaningful rhetoric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe28a88",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_tfidf_terms(response, feature_names, top_n=20):\n",
        "    # Sort the indices of the response rows (documents) by their score in descending order\n",
        "    sorted_nzs = np.argsort(response.data)[::-1]\n",
        "    return feature_names[response.indices[sorted_nzs]][:top_n], response.data[sorted_nzs][:top_n]\n",
        "\n",
        "top_terms = {}\n",
        "\n",
        "for i, candidate in enumerate(candidates):\n",
        "    doc_vector = tfidf_matrix[i, :]\n",
        "    terms, scores = get_top_tfidf_terms(doc_vector, feature_names, top_n=20)\n",
        "    top_terms[candidate] = pd.DataFrame({'term': terms, 'score': scores})\n",
        "    \n",
        "    print(f\"--- {candidate} ---\")\n",
        "    print(top_terms[candidate])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c07f0f5",
      "metadata": {},
      "source": [
        "## 3. Visualization\n",
        "\n",
        "Visualizing the top distinctive terms for each candidate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4c3505",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(20, 10), sharex=False)\n",
        "\n",
        "for i, candidate in enumerate(candidates):\n",
        "    df_plot = top_terms[candidate]\n",
        "    sns.barplot(x='score', y='term', data=df_plot, ax=axes[i], palette='viridis', hue='term', legend=False)\n",
        "    axes[i].set_title(f'Top TF-IDF Terms: {candidate}', fontsize=16)\n",
        "    axes[i].set_xlabel('TF-IDF Score')\n",
        "    axes[i].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "The analysis highlights the distinctive vocabulary of each candidate. \n",
        "- **Trump's** distinctive terms likely revolve around his campaign themes (e.g., \"wall\", \"border\", specific opponents).\n",
        "- **Biden's** terms may focus on policy or specific phrases he uses often (e.g., \"folks\", \"jobs\").\n",
        "- **Harris's** terms will reflect her specific campaign messaging.\n",
        "\n",
        "Note: `max_df=0.7` in the vectorizer helps remove words that are very common across all three candidates (like \"president\", \"country\", \"people\"), ensuring we see the *differences*."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
