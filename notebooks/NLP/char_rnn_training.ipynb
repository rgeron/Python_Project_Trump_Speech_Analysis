{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CHAR-RNN Training on Trump Speeches\n",
        "\n",
        "This notebook trains a Character-RNN model on Trump speeches dataset using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.8.0\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total text length: 1000000 characters\n",
            "Sample: well that be good timing be not it we have to get that right we have to get that\n"
          ]
        }
      ],
      "source": [
        "# Load the parquet file\n",
        "df = pd.read_parquet('../../data/transcriptions.parquet')\n",
        "\n",
        "# Use the specified column\n",
        "text_column = 'clean-v1-with-stopwords'\n",
        "\n",
        "# Concatenate all text\n",
        "full_text = \" \".join(df[text_column].dropna().astype(str).tolist())\n",
        "\n",
        "# Limit to 1M characters for faster training/testing\n",
        "full_text = full_text[:1000000]\n",
        "\n",
        "print(f\"Total text length: {len(full_text)} characters\")\n",
        "print(f\"Sample: {full_text[:80]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing & Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of distinct characters: 55\n"
          ]
        }
      ],
      "source": [
        "# Create a mapping from character to integer\n",
        "chars = sorted(list(set(full_text)))\n",
        "vocab_size = len(chars)\n",
        "char2idx = {c: i for i, c in enumerate(chars)}\n",
        "idx2char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "# Encode the text\n",
        "encoded = np.array([char2idx[c] for c in full_text])\n",
        "\n",
        "print(f\"Number of distinct characters: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Creation Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building datasets...\n"
          ]
        }
      ],
      "source": [
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, seq_length):\n",
        "        self.data = data\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        # We define length as number of possible windows\n",
        "        return len(self.data) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Slicing numpy array is fast\n",
        "        chunk = self.data[idx:idx + self.seq_length + 1]\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "# Create Training, Validation, and Test sets\n",
        "seq_length = 100\n",
        "batch_size = 128  # Increased batch size for efficiency\n",
        "\n",
        "dataset_size = len(encoded)\n",
        "train_size = int(dataset_size * 0.9)\n",
        "valid_size = int(dataset_size * 0.05)\n",
        "\n",
        "# Split data indices\n",
        "train_data = encoded[:train_size]\n",
        "valid_data = encoded[train_size:train_size+valid_size]\n",
        "test_data = encoded[train_size+valid_size:]\n",
        "\n",
        "print(\"Building datasets...\")\n",
        "train_dataset = CharDataset(train_data, seq_length)\n",
        "valid_dataset = CharDataset(valid_data, seq_length)\n",
        "test_dataset = CharDataset(test_data, seq_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build and Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building model...\n",
            "Starting training...\n",
            "Epoch 1, Step 0, Loss: 4.0157\n",
            "Epoch 1, Step 1000, Loss: 1.3066\n",
            "Epoch 1, Step 2000, Loss: 1.1754\n",
            "Epoch 1, Step 3000, Loss: 1.1556\n",
            "Epoch 1, Step 4000, Loss: 1.1733\n",
            "Epoch 1, Step 5000, Loss: 1.1214\n",
            "Epoch 1, Step 6000, Loss: 1.1201\n",
            "Epoch 1, Step 7000, Loss: 1.1105\n",
            "Epoch 1 Complete. Avg Train Loss: 1.2130, Val Loss: 1.1538\n"
          ]
        }
      ],
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.gru(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "print(\"Building model...\")\n",
        "model = CharRNN(vocab_size, embed_dim=16, hidden_dim=128).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.NAdam(model.parameters())\n",
        "\n",
        "print(\"Starting training...\")\n",
        "num_epochs = 1  # Kept low for demonstration\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(x)\n",
        "        \n",
        "        # Reshape for loss: (N*L, C) vs (N*L)\n",
        "        loss = criterion(output.reshape(-1, vocab_size), y.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Step {i}, Loss: {loss.item():.4f}\")\n",
        "            \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in valid_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            output, _ = model(x)\n",
        "            loss = criterion(output.reshape(-1, vocab_size), y.reshape(-1))\n",
        "            val_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} Complete. Avg Train Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(valid_loader):.4f}\")\n",
        "    \n",
        "    # Save checkpoint\n",
        "    torch.save(model.state_dict(), \"char_rnn_trump.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inference & Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction test (we will make america...):\n",
            "we will make america be go ahoot you believe in for a hot but it 's probably run from they thank you the thank you japag\n",
            "\n",
            "--- Temperature 0.2 (Conservative) ---\n",
            "we will make america be they be go to be a great job they be go to be a great job and I say that 's what 's go to be a g\n",
            "\n",
            "--- Temperature 1.0 (Balanced) ---\n",
            "we will make america no not do I do not even fight that would victure it on get them but where different mean agricia th\n",
            "\n",
            "--- Temperature 2.0 (Chaotic) ---\n",
            "we will make america holaken Ninum techarby nicemy coutoriverson technoersulvasfate who to I he zignse withsete belive l\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, start_string, num_generate=100, temperature=1.0):\n",
        "    model.eval()\n",
        "    # Convert start string to numbers\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = torch.tensor(input_eval, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    \n",
        "    text_generated = []\n",
        "    hidden = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_generate):\n",
        "            # forward pass\n",
        "            output, hidden = model(input_eval, hidden)\n",
        "            \n",
        "            # Take the last time step prediction\n",
        "            predictions = output[:, -1, :]\n",
        "            predictions = predictions / temperature\n",
        "            probs = torch.softmax(predictions, dim=-1)\n",
        "            \n",
        "            # Sample\n",
        "            predicted_id = torch.multinomial(probs, num_samples=1).item()\n",
        "            \n",
        "            # Pass the predicted character as the next input\n",
        "            input_eval = torch.tensor([[predicted_id]], device=device)\n",
        "            \n",
        "            text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "# --- Run Generation ---\n",
        "# model.load_state_dict(torch.load(\"my_trump_model.pth\"))\n",
        "\n",
        "print(\"Prediction test (we will make america...):\")\n",
        "print(generate_text(model, \"we will make america\", num_generate=100))\n",
        "\n",
        "print(\"\\n--- Temperature 0.2 (Conservative) ---\")\n",
        "print(generate_text(model, \"we will make america\", num_generate=100, temperature=0.2))\n",
        "\n",
        "print(\"\\n--- Temperature 1.0 (Balanced) ---\")\n",
        "print(generate_text(model, \"we will make america\", num_generate=100, temperature=1.0))\n",
        "\n",
        "print(\"\\n--- Temperature 2.0 (Chaotic) ---\")\n",
        "print(generate_text(model, \"we will make america\", num_generate=100, temperature=2.0))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
