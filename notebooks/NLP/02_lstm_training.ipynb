{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 2: LSTM Text Generation with PyTorch\n",
                "\n",
                "In this notebook, we will train a Long Short-Term Memory (LSTM) network to generate text in the style of Donald Trump.\n",
                "We will use the `TextDataset` class we created in `src/nlp/data_loader.py`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "sys.path.append(os.path.abspath('../../src'))\n",
                "\n",
                "from nlp.data_loader import TextDataset, get_vocab_size"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "SEQUENCE_LENGTH = 10\n",
                "BATCH_SIZE = 64\n",
                "EMBEDDING_DIM = 128\n",
                "HIDDEN_DIM = 256\n",
                "NUM_LAYERS = 2\n",
                "LEARNING_RATE = 0.001\n",
                "EPOCHS = 5\n",
                "MAX_VOCAB_SIZE = 5000\n",
                "\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "MemoryError",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load Dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset = \u001b[43mTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../../data/transcriptions_cleaned.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEQUENCE_LENGTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_vocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_VOCAB_SIZE\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      9\u001b[39m vocab_size = get_vocab_size(dataset)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nicol\\Getting in Trump s Head\\Getting-in-Trump-s-Head\\src\\nlp\\data_loader.py:29\u001b[39m, in \u001b[36mTextDataset.__init__\u001b[39m\u001b[34m(self, parquet_path, sequence_length, max_vocab_size)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mself\u001b[39m.int_to_word = {i: word \u001b[38;5;28;01mfor\u001b[39;00m word, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.word_to_int.items()}\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Convert text to integers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28mself\u001b[39m.encoded_text = [\u001b[38;5;28mself\u001b[39m.word_to_int[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.word_to_int]\n",
                        "\u001b[31mMemoryError\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "\n",
                "dataset = TextDataset(\n",
                "    parquet_path='../../data/transcriptions_cleaned.parquet', \n",
                "    sequence_length=SEQUENCE_LENGTH,\n",
                "    max_vocab_size=MAX_VOCAB_SIZE\n",
                ")\n",
                "\n",
                "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "vocab_size = get_vocab_size(dataset)\n",
                "print(f\"Vocabulary Size: {vocab_size}\")\n",
                "print(f\"Total Sequences: {len(dataset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "class LSTMGenerator(nn.Module):\n",
                "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
                "        super(LSTMGenerator, self).__init__()\n",
                "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
                "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
                "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
                "        \n",
                "    def forward(self, x, hidden):\n",
                "        embeds = self.embedding(x)\n",
                "        out, hidden = self.lstm(embeds, hidden)\n",
                "        out = out[:, -1, :]\n",
                "        \n",
                "        out = self.fc(out)\n",
                "        return out, hidden\n",
                "    \n",
                "    def init_hidden(self, batch_size):\n",
                "        return (torch.zeros(NUM_LAYERS, batch_size, HIDDEN_DIM).to(device),\n",
                "                torch.zeros(NUM_LAYERS, batch_size, HIDDEN_DIM).to(device))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = LSTMGenerator(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS).to(device)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/5, Batch 0, Loss: 8.5251\n",
                        "Epoch 1/5, Batch 100, Loss: 6.5810\n",
                        "Epoch 1/5, Batch 200, Loss: 6.2387\n",
                        "Epoch 1/5, Batch 300, Loss: 5.8398\n",
                        "Epoch 1/5, Batch 400, Loss: 6.0487\n",
                        "Epoch 1/5, Batch 500, Loss: 6.5078\n",
                        "Epoch 1/5, Batch 600, Loss: 6.2709\n",
                        "Epoch 1/5, Batch 700, Loss: 5.8747\n",
                        "Epoch 1/5, Batch 800, Loss: 6.1920\n",
                        "Epoch 1/5, Batch 900, Loss: 6.6175\n",
                        "Epoch 1/5, Batch 1000, Loss: 5.6697\n",
                        "Epoch 1/5, Batch 1100, Loss: 5.4491\n",
                        "Epoch 1/5, Batch 1200, Loss: 5.5430\n",
                        "Epoch 1/5, Batch 1300, Loss: 5.5711\n",
                        "Epoch 1/5, Batch 1400, Loss: 5.3918\n",
                        "Epoch 1/5, Batch 1500, Loss: 5.8156\n",
                        "Epoch 1/5, Batch 1600, Loss: 6.0520\n",
                        "Epoch 1/5, Batch 1700, Loss: 5.8310\n",
                        "Epoch 1/5, Batch 1800, Loss: 5.3203\n",
                        "Epoch 1/5, Batch 1900, Loss: 6.0250\n",
                        "Epoch 1/5, Batch 2000, Loss: 5.7461\n",
                        "Epoch 1/5, Batch 2100, Loss: 6.0078\n",
                        "Epoch 1/5, Batch 2200, Loss: 5.3119\n",
                        "Epoch 1/5, Batch 2300, Loss: 5.9969\n",
                        "Epoch 1/5, Batch 2400, Loss: 5.5040\n",
                        "Epoch 1/5, Batch 2500, Loss: 5.6189\n",
                        "Epoch 1/5, Batch 2600, Loss: 4.8527\n",
                        "Epoch 1/5, Batch 2700, Loss: 5.0092\n",
                        "Epoch 1/5, Batch 2800, Loss: 5.0549\n",
                        "Epoch 1/5, Batch 2900, Loss: 5.7645\n",
                        "Epoch 1/5, Batch 3000, Loss: 5.2969\n",
                        "Epoch 1/5, Batch 3100, Loss: 5.9772\n",
                        "Epoch 1/5, Batch 3200, Loss: 5.0940\n",
                        "Epoch 1/5, Batch 3300, Loss: 5.3582\n",
                        "Epoch 1/5, Batch 3400, Loss: 5.3059\n",
                        "Epoch 1/5, Batch 3500, Loss: 5.2914\n",
                        "Epoch 1/5, Batch 3600, Loss: 5.1541\n",
                        "Epoch 1/5, Batch 3700, Loss: 5.2424\n",
                        "Epoch 1/5, Batch 3800, Loss: 4.7689\n",
                        "Epoch 1/5, Batch 3900, Loss: 4.7917\n",
                        "Epoch 1/5, Batch 4000, Loss: 5.2884\n",
                        "Epoch 1/5, Batch 4100, Loss: 4.8818\n",
                        "Epoch 1/5, Batch 4200, Loss: 5.1353\n",
                        "Epoch 1/5, Batch 4300, Loss: 4.7260\n",
                        "Epoch 1/5, Batch 4400, Loss: 4.9388\n"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    for batch_idx, (x, y) in enumerate(dataloader):\n",
                "        x, y = x.to(device), y.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        hidden = model.init_hidden(x.size(0))\n",
                "        output, _ = model(x, hidden)\n",
                "        \n",
                "        loss = criterion(output, y)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        \n",
                "        if batch_idx % 100 == 0:\n",
                "            print(f\"Epoch {epoch+1}/{EPOCHS}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
                "            \n",
                "    print(f\"Epoch {epoch+1} Complete. Average Loss: {total_loss / len(dataloader):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_text(model, start_text, length=20):\n",
                "    model.eval()\n",
                "    words = start_text.lower().split()\n",
                "    state_h, state_c = model.init_hidden(1)\n",
                "    \n",
                "    for _ in range(length):\n",
                "        if len(words) < SEQUENCE_LENGTH:\n",
                "             input_seq = [dataset.word_to_int.get(w, 0) for w in words]\n",
                "             while len(input_seq) < SEQUENCE_LENGTH:\n",
                "                 input_seq.insert(0, 0) \n",
                "        else:\n",
                "            input_seq = [dataset.word_to_int.get(w, 0) for w in words[-SEQUENCE_LENGTH:]]\n",
                "            \n",
                "        x = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            output, (state_h, state_c) = model(x, (state_h, state_c))\n",
                "            \n",
                "        probs = torch.nn.functional.softmax(output[0], dim=0)\n",
                "        next_word_idx = torch.multinomial(probs, 1).item()\n",
                "        next_word = dataset.int_to_word[next_word_idx]\n",
                "        \n",
                "        words.append(next_word)\n",
                "        \n",
                "    return \" \".join(words)\n",
                "\n",
                "print(generate_text(model, \"Make America Great\", 20))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
