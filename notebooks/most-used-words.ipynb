{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Most Used Words Analysis\n",
        "\n",
        "This notebook analyzes the most used words in Donald Trump's speeches using the `SpeechCorpus` tool. We will filter the speeches by various criteria (Campaign, Rally, Location) and display the top 10 most frequent words for each subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(str(project_root / \"src\"))\n",
        "\n",
        "from filtering_corpus.speech_corpus import SpeechCorpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_top_words(corpus, n=3, stop_words=None):\n",
        "    \"\"\"\n",
        "    Calculates the top n most used words in the corpus transcriptions.\n",
        "    \"\"\"\n",
        "    if stop_words is None:\n",
        "        stop_words = set(['the', 'don', 'applause', 'and', 'to', 'of', 'a', 'in', 'that', 'is', 'i', 'for', 'it', 'you', 'we', 'are', 'on', 'this', 'have', 'be', 'with', 'they', 'as', 'not', 'will', 'at', 'our', 'my', 'was', 'but', 'by', 'he', 'she', 'so', 'what', 'all', 'if', 'their', 'who', 'me', 'or', 'do', 'has', 'from', 'an', 'no', 'one', 'would', 'there', 'can', 'about', 'just', 'out', 'up', 'when', 'like', 'them', 'your', 'go', 'get', 'know', 'very', 'going', 'people', 'because', 'now', 'had', 'were', 'been', 'than', 'back', 'see', 'time', 'some', 'could', 'did', 'make', 'us', 'said', 'say', 'got', 'him', 'his', 'her', 'down', 'only', 'want', 'think', 'right', 'look', 'take', 'way', 'how', 'come', 'its', 'over', 'then', 'also', 'even', 'much', 'more', 'these', 'those', 'where', 'why', 'which', 'here', 'well', 'many', 'other', 'really', 'too', 'should', 'never', 'good', 'great', 'big', 'lot', 'thing', 'things'])\n",
        "\n",
        "    all_text = \" \".join(corpus.transcriptions['text'].astype(str))\n",
        "    \n",
        "    # Simple tokenization (remove punctuation and lowercase)\n",
        "    words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
        "    \n",
        "    # Filter stop words and short words\n",
        "    filtered_words = [w for w in words if w not in stop_words and len(w) > 2]\n",
        "    \n",
        "    counter = Counter(filtered_words)\n",
        "    return counter.most_common(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Corpus\n",
        "\n",
        "We'll try to use the cleaned transcriptions if available, otherwise fallback to raw."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8b0ce579",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using transcriptions.\n",
            "Total speeches: 894\n"
          ]
        }
      ],
      "source": [
        "corpus = SpeechCorpus(data_dir=\"../data\", transcription_file=\"transcriptions.parquet\")\n",
        "print(\"Using transcriptions.\")\n",
        "\n",
        "print(f\"Total speeches: {len(corpus.speeches)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis by Campaign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 2016 Campaign (Speeches: 306) ---\n",
            "country: 6931\n",
            "gonna: 4436\n",
            "love: 3718\n",
            "\n",
            "--- 2020 Campaign (Speeches: 153) ---\n",
            "thank: 4304\n",
            "years: 4256\n",
            "country: 3885\n",
            "\n",
            "--- 2024 Campaign (Speeches: 227) ---\n",
            "country: 10561\n",
            "thank: 6561\n",
            "years: 5602\n"
          ]
        }
      ],
      "source": [
        "campaigns = [\"2016\", \"2020\", \"2024\"]\n",
        "\n",
        "for campaign in campaigns:\n",
        "    sub_corpus = corpus.get_campaign(campaign)\n",
        "    top_words = get_top_words(sub_corpus)\n",
        "    print(f\"\\n--- {campaign} Campaign (Speeches: {len(sub_corpus.speeches)}) ---\")\n",
        "    for word, count in top_words:\n",
        "        print(f\"{word}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis: Rallies vs Non-Rallies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Rallies (Speeches: 380) ---\n",
            "country: 14826\n",
            "thank: 9580\n",
            "years: 9025\n",
            "\n",
            "--- Non-Rallies (Speeches: 514) ---\n",
            "country: 12182\n",
            "thank: 8752\n",
            "years: 6899\n"
          ]
        }
      ],
      "source": [
        "rallies = corpus.get_rallies(is_rally=True)\n",
        "non_rallies = corpus.get_rallies(is_rally=False)\n",
        "\n",
        "print(f\"\\n--- Rallies (Speeches: {len(rallies.speeches)}) ---\")\n",
        "for word, count in get_top_words(rallies):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(f\"\\n--- Non-Rallies (Speeches: {len(non_rallies.speeches)}) ---\")\n",
        "for word, count in get_top_words(non_rallies):\n",
        "    print(f\"{word}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis by Location: Pennsylvania"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Pennsylvania Speeches (Speeches: 43) ---\n",
            "country: 1835\n",
            "years: 1164\n",
            "pennsylvania: 1102\n"
          ]
        }
      ],
      "source": [
        "pa_corpus = corpus.get_by_location(\"Pennsylvania\")\n",
        "print(f\"\\n--- Pennsylvania Speeches (Speeches: {len(pa_corpus.speeches)}) ---\")\n",
        "for word, count in get_top_words(pa_corpus):\n",
        "    print(f\"{word}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis by Category: Economy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Economy Speeches (Speeches: 169) ---\n",
            "country: 4585\n",
            "thank: 2968\n",
            "years: 2613\n"
          ]
        }
      ],
      "source": [
        "economy_corpus = corpus.get_by_category(\"Economy\")\n",
        "print(f\"\\n--- Economy Speeches (Speeches: {len(economy_corpus.speeches)}) ---\")\n",
        "for word, count in get_top_words(economy_corpus):\n",
        "    print(f\"{word}: {count}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
